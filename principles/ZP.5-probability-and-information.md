# ZP.5 — Вероятность и информация

## Определение

Сложные системы описывай через вероятности, энтропию и информацию.

## Следствие

Не «X произойдёт», а «X произойдёт с вероятностью P при условиях C». Неопределённость — не враг, а объект описания.

## Примеры применения

**1. Обучение = сжатие (Kolmogorov/MDL, 2024--2025).**
"Bridging Kolmogorov Complexity and Deep Learning" (сентябрь 2025): асимптотически оптимальные объективы длины описания для трансформеров, основанные на сложности Колмогорова. "Complexity Dynamics of Grokking" (2025): фазовый переход от запоминания к обобщению = резкое падение сложности. Обучение — это открытие более коротких программ, генерирующих те же наблюдения.

**2. Generalized Information Bottleneck (2025).**
GIB переформулирует принцип узкого горлышка через СИНЕРГИЮ — информацию, получаемую только при совместной обработке признаков. Синергетические функции достигают лучшего обобщения. GIB показывает фазы сжатия даже для ReLU-архитектур и трансформеров.

**3. Статистическое обучение младенцев (Saffran, 1996 → 2024).**
8-месячные младенцы автоматически извлекают вероятности переходов из непрерывных потоков речи, отбрасывая нерелевантную вариацию (голос, скорость, высота) и сохраняя структурные паттерны (границы слов). Information Bottleneck в действии: сжатие высокоразмерного слухового входа в низкоразмерные структурные представления.

**4. IIT 4.0 — обновление теории интегрированной информации (2023--2025).**
IIT 4.0 вводит уникальные меры внутренней информации, согласованные с 5 аксиомами феноменального существования. Тождество: опыт = причинно-следственная структура, развёрнутая из максимального субстрата. Критика (Erkenntnis, 2025): IIT игнорирует роль внимания — информация без механизма отбора недостаточна.

**5. Конформное предсказание для каузальных эффектов (2024--2025).**
Предиктивные интервалы без предположений о распределении для контрфактических исходов, даже при скрытых конфаундерах. 14-й симпозиум по Conformal Prediction (Лондон, сентябрь 2025). Практика: транспорт, медицина.

**6. Объединение Шеннон ↔ Колмогоров (2024--2026).**
Все линейные неравенства, верные для сложности Колмогорова, верны для энтропии Шеннона и наоборот. Однако аналогия разрушается для нелинейных утверждений. Специальный выпуск Entropy (2024--2025).

## Типичные нарушения

**N1. IIT 4.0 игнорирует внимание (Erkenntnis, 2025).** Трактуя сознание как чисто информационно-теоретическое (интегрированная информация) без учёта вычислительного процесса внимания, IIT делает предсказания, противоречащие феноменологическим данным. Информация без механизма отбора — недостаточна.

**N2. Конформное предсказание при сдвиге распределения.** Стандартное конформное предсказание предполагает обменяемость (exchangeability), которая нарушается при сдвиге распределения. Применение вероятностных гарантий от одного распределения к другому — нарушение принципа.

**N3. p < 0.05 как «доказательство» (кризис воспроизводимости).** Когда исследователи трактуют p < 0.05 как доказательство, а не как вероятностное свидетельство, они нарушают ZP.5. Кризис воспроизводимости — частично ошибка базовой частоты: априорная вероятность гипотезы игнорируется.

**N4. Overfit (запоминание без сжатия).** Модель, которая запоминает обучающие данные без их сжатия (без извлечения достаточных статистик), нарушает информационный принцип. Regularization = принуждение к сжатию.

## Проявление в развитии ребёнка

Младенцы — байесовские рассуждатели с рождения. В 12 месяцев превербальные младенцы формируют изменяющиеся во времени ожидания будущих событий, согласующиеся с байесовским идеальным наблюдателем (Science, 2011). В 4.5--6 месяцев младенцы используют пропорциональное (вероятностное) рассуждение. К 4--5 годам дети делают предиктивные выводы в условиях неопределённости, но с трудом справляются с диагностическими (обратными), требующими полного байесовского рассуждения. Критично: когда информация представлена в натуральных частотах (а не вероятностях), даже маленькие дети решают байесовские задачи правильно — формат вероятностной информации важен не менее её содержания.

## Связь с FPF

Trust Calculus (F + G + R), TGA. Доверие к утверждениям выражается количественно: факты (F), обоснования (G), репутация (R) — всё через вероятностные оценки. WLNK (Weakest-Link): R_eff = max(0, min_i(R_i) − Φ(CL_min)) — никакого усреднения, слабейшее звено определяет целое. KD-CAL: знания стареют, доверие убывает без обновления — энтропия знания.

## Ключевые исследователи (2024--2026)

- Giulio Tononi (UW-Madison) — IIT 4.0
- Vladimir Vovk (Royal Holloway) — конформное предсказание
- Judea Pearl (UCLA) — каузальный вывод, Pearl Causal Hierarchy
- Naftali Tishby (посмертно) — Information Bottleneck (1999)
- Noga Zaslavsky (MIT) — эффективное кодирование, IB в когниции
- Josh Tenenbaum (MIT) — байесовские модели когнитивного развития
